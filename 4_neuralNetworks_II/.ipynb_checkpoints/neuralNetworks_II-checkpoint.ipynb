{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.image import NonUniformImage\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "pathToDataFile = 'ex4data1.mat'\n",
    "data = loadmat(pathToDataFile)\n",
    "pathToWeightsFile = 'ex4weights.mat'\n",
    "weights = loadmat(pathToWeightsFile)\n",
    "print(weights['Theta1'].shape)\n",
    "print(weights['Theta2'].shape)\n",
    "print(type(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Nural Network\n",
    "## 1.1 Forward Porpagation\n",
    "<img src=\"forwardPropagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData(pathToDataFile):\n",
    "    data = loadmat(pathToDataFile)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "def generateBeta(layer):\n",
    "    '''Generate beta-matrix for every layer in Neural Network'''\n",
    "    betaSet = ()\n",
    "    for i in range(len(layer)-1):\n",
    "#         recommendation from Andrew Ng window is ±(6/(inLayer + outLayer))**0.5\n",
    "        low, high = -(6/(layer[i]+layer[i+1]))**0.5, (6/(layer[i]+layer[i+1]))**0.\n",
    "        betaSet += (np.random.uniform(low,high,(layer[i+1], layer[i]+1)),)\n",
    "#         betaSet += (np.zeros((outLayer, inLayer+1)),)\n",
    "    return betaSet\n",
    "\n",
    "def flattenBeta(betaSet):\n",
    "    flatBeta = betaSet[0].flatten()\n",
    "    for beta in betaSet[1:]:\n",
    "        flatBeta = np.concatenate((flatBeta, beta.flatten()), axis=-1)\n",
    "    return flatBeta\n",
    "\n",
    "def reshapeBeta(Beta, layer):\n",
    "    splitIndex = 0\n",
    "    splitIndices = []\n",
    "    for i in range(len(layer)-2):\n",
    "        splitIndex += (layer[i]+1)*layer[i+1]\n",
    "        splitIndices += [splitIndex]\n",
    "    splitBeta = np.split(Beta, splitIndices)\n",
    "    reshapedBeta = ()\n",
    "    for i in range(len(splitBeta)):\n",
    "        reshapedBeta += (splitBeta[i].reshape(layer[i+1],layer[i]+1),)\n",
    "    return reshapedBeta\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forwardPropagation(flatBeta, layer, flatX, sampleSize):\n",
    "    '''Forward Propagation is the hypothesis function for Neural Networks'''\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "#     H_0 (5000, 400)\n",
    "    H = flatX.reshape(sampleSize, -1)\n",
    "#     Z_H = ()\n",
    "    H_byLayer = ()\n",
    "    for beta in betaSet:\n",
    "#         print(H.shape)\n",
    "#         Z_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        Z = np.dot(np.insert(H, 0, 1, axis=1), beta.T)\n",
    "#         H_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        H = sigmoid(Z)\n",
    "#         Z_H += ((Z, H),)\n",
    "        H_byLayer += (H,)\n",
    "#     H_2 (5000, 10)\n",
    "    return H_byLayer\n",
    "\n",
    "def sigmoidGradient(Z):\n",
    "    return sigmoid(Z)*(1-sigmoid(Z))\n",
    "\n",
    "def costFunction(flatBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    X = flatX.reshape(sampleSize, -1)\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "    J = 0\n",
    "    for n in range(sampleSize):\n",
    "        x_n = X[n:n+1,:]\n",
    "        y_n = Y[n:n+1,:]\n",
    "#         hypothesis vector h_n(1, 10)\n",
    "        h_n = forwardPropagation(flatBeta, layer, x_n, 1)[len(betaSet)-1]\n",
    "#         cost function scalar j_n(1, 1) = y_n(1, 10)*h_n.T(10, 1)\n",
    "        j_n = (- np.dot(y_n, np.log(h_n).T) - np.dot((1-y_n), np.log(1-h_n).T))\n",
    "        J += j_n\n",
    "#     regularisation term (R)\n",
    "    cummulativeR = 0\n",
    "    for beta in betaSet:\n",
    "        cummulativeR += np.sum(beta*beta) #element-wise multiplication\n",
    "    cummulativeR *= iLambda/(2*sampleSize)\n",
    "    return J[0][0]/sampleSize + cummulativeR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Neural Network Initialisation\n",
    "To initialise a simple neural network, one has to do the following:\n",
    "1. set the number of neurons in every layer (including input and output layers)\n",
    "2. extract and flatten input matrix X\n",
    "3. transform output Y\n",
    "3. initialise Beat matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "# Set number of neurons in every layer (including input and output layers)\n",
    "layer = 400, 25, 10\n",
    "# Extract and flatten input matrix X\n",
    "X, y = getData(pathToDataFile)\n",
    "sampleSize, numVariables = X.shape\n",
    "flatX = X.flatten()\n",
    "yUnique = np.unique(y)\n",
    "# Initialise Beat matrix\n",
    "betaTest = flattenBeta((weights['Theta1'], weights['Theta2']))\n",
    "betaInitial = flattenBeta(generateBeta(layer))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "for beta in generateBeta(layer): print(beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28762916516131876"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iLambda = 0\n",
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "betaSet = reshapeBeta(betaTest, layer)\n",
    "J = 0\n",
    "for n in range(sampleSize):\n",
    "    x_n = X[n:n+1,:]\n",
    "    y_n = Y[n:n+1,:]\n",
    "#         hypothesis vector h_n(1, 10)\n",
    "    h_n = forwardPropagation(betaTest, layer, x_n, 1)[len(betaSet)-1]\n",
    "#         cost function scalar j_n(1, 1) = y_n(1, 10)*h_n.T(10, 1)\n",
    "    j_n = (- np.dot(y_n, np.log(h_n).T) - np.dot((1-y_n), np.log(1-h_n).T))\n",
    "    J += j_n\n",
    "#     regularisation term (R)\n",
    "cummulativeR = 0\n",
    "for beta in betaSet:\n",
    "    cummulativeR += np.sum(beta*beta) #element-wise multiplication\n",
    "cummulativeR *= iLambda/(2*sampleSize)\n",
    "J[0][0]/sampleSize + cummulativeR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Forward-Propagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "print(Y[0:0+1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "print(forwardPropagation(betaTest, layer, flatX, sampleSize)[1].shape)\n",
    "print(forwardPropagation(betaTest, layer, X[0:0+1,:], 1)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(1, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11589058107504865"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0][None,:].shape)\n",
    "# costFunction(betaTest, layer, X.flatten(), sampleSize, y, yUnique, iLambda = 0.)\n",
    "costFunction(betaTest, layer, X[0:100][None,:].flatten(), 100, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Cost-Function Test\n",
    "The outputs of the costFunction should be as follows:<br\\>\n",
    "betaTest, X, iLambda=0. — 0.287629165161<br\\>\n",
    "betaTest, X, iLambda=1. — 0.384487796243<br\\>\n",
    "betaTest, X, iLambda=0. — 0.0345203898838<br\\>\n",
    "betaInitial, X, iLambda=1. — 65.5961451562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n",
      "0.384487796243\n",
      "0.0345203898838\n",
      "65.5961451562\n"
     ]
    }
   ],
   "source": [
    "print(costFunction(betaTest, layer, flatX, sampleSize, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(betaTest, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))\n",
    "print(costFunction(betaTest, layer, X[0][None,:].flatten(), 1, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(betaInitial, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backPropagation(flatBeta, layer, flatX, sampleSize, y, yUnique):\n",
    "    sampleSize, numVariables = X.shape\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "\n",
    "    deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "    H = forwardPropagation(flatBeta, layer, flatX, sampleSize)\n",
    "#     error matrix E(5000, 10)\n",
    "    E = H[len(layer)-2] - Y\n",
    "    for l in reversed(range(len(layer)-1)):\n",
    "        E = np.dot(E*sigmoidGradient(H[l]), betaSet[l])[:,1:]\n",
    "        deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "    flatDelta = flattenBeta(deltaSet)\n",
    "    return flatBeta + flatDelta/sampleSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16253.0530028\n",
      "199.953706643\n"
     ]
    }
   ],
   "source": [
    "betaInitial = flattenBeta(generateBeta(layer))\n",
    "a = backPropagation(betaInitial, layer, flatX, sampleSize, y, yUnique)\n",
    "\n",
    "print(np.sum(a))\n",
    "print(costFunction(a,layer, flatX, sampleSize, y, yUnique, iLambda = 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8423308979e-06\n",
      "8.10018718767e-09\n",
      "8.52651282912e-10\n",
      "1.1226575225e-08\n",
      "0.0\n",
      "0.0\n",
      "7.1054273576e-11\n",
      "0.0\n",
      "3.86989995604e-06\n",
      "1.06581410364e-08\n"
     ]
    }
   ],
   "source": [
    "def gradientCheck(beta, layer, sampleSize, epsilon):\n",
    "    for i in np.random.randint(beta.size, size=10):\n",
    "        epsilonVector = np.zeros(beta.size)\n",
    "        epsilonVector[i] = epsilon\n",
    "        betaPlus = betaMinus = beta\n",
    "        betaPlus = betaPlus + epsilonVector\n",
    "        costPlus = costFunction(betaMinus,layer, X, sampleSize, y, yUnique, iLambda = 0.)\n",
    "        betaMinus = betaMinus - epsilonVector\n",
    "        costMinus = costFunction(betaMinus,layer, X, sampleSize, y, yUnique, iLambda = 0.)\n",
    "        approximateGradient = (costPlus-costMinus)/(2*epsilon)\n",
    "        print (approximateGradient)\n",
    "\n",
    "epsilon = 0.0001\n",
    "gradientCheck(betaInitial, layer, sampleSize, epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_1(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "    optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                      method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "#     optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "#                                      args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                      maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_2(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "#     optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                       method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "    optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "                                     args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                     maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = betaOptimisation_1(betaInitial, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = betaOptimisation_2(betaInitial, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qualityControl(optimisedBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    X = flatX.reshape(sampleSize,-1)\n",
    "    yAssignmentVector = []\n",
    "    misAssignedIndex = []\n",
    "    for n in range(sampleSize):\n",
    "        x = X[n]\n",
    "        yAssignment =  np.argmax(forwardPropagation(optimisedBeta, layer, X[n], 1)[1]) + 1\n",
    "        if yAssignment == y[n]:\n",
    "            yAssignmentVector += [True]\n",
    "        else:\n",
    "            yAssignmentVector += [False]\n",
    "            misAssignedIndex += [n]\n",
    "    return (sum(yAssignmentVector)/sampleSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neuralNetworkClassifier(, flatX, sampleSize, y, yUnique, iLambda=0.)\n",
    "qualityControl(a['x'], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qualityControl(b[0], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
