{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.image import NonUniformImage\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "pathToDataFile = 'ex4data1.mat'\n",
    "data = loadmat(pathToDataFile)\n",
    "pathToWeightsFile = 'ex4weights.mat'\n",
    "weights = loadmat(pathToWeightsFile)\n",
    "print(weights['Theta1'].shape)\n",
    "print(weights['Theta2'].shape)\n",
    "print(type(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Nural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neuralNetwork.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData(pathToDataFile):\n",
    "    data = loadmat(pathToDataFile)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "# def flattenX(X):\n",
    "#     flatX = X.flatten()\n",
    "#     return flatX\n",
    "\n",
    "# def reshapeX(flatX, sampleSize):\n",
    "#     reshapedX = flatX.reshape(sampleSize,-1)\n",
    "#     return reshapedX\n",
    "\n",
    "def generateBeta(layer):\n",
    "    '''Generate beta-matrix for every layer in Neural Network'''\n",
    "    betaSet = ()\n",
    "    for i in range(len(layer)-1):\n",
    "#         recommendation from Andrew Ng window is ±(6/(inLayer + outLayer))**0.5\n",
    "        low, high = -(6/(layer[i]+layer[i+1]))**0.5, (6/(layer[i]+layer[i+1]))**0.\n",
    "        betaSet += (np.random.uniform(low,high,(layer[i+1], layer[i]+1)),)\n",
    "#         betaSet += (np.zeros((outLayer, inLayer+1)),)\n",
    "    return betaSet\n",
    "\n",
    "def flattenBeta(betaSet):\n",
    "    flatBeta = betaSet[0].flatten()\n",
    "    for beta in betaSet[1:]:\n",
    "        flatBeta = np.concatenate((flatBeta, beta.flatten()), axis=-1)\n",
    "    return flatBeta\n",
    "\n",
    "def reshapeBeta(flatBeta, layer):\n",
    "    splitIndex = 0\n",
    "    splitIndices = []\n",
    "    for i in range(len(layer)-2):\n",
    "        splitIndex += (layer[i]+1)*layer[i+1]\n",
    "        splitIndices += [splitIndex]\n",
    "    splitBeta = np.split(flatBeta, splitIndices)\n",
    "    reshapedBeta = ()\n",
    "    for i in range(len(splitBeta)):\n",
    "        reshapedBeta += (splitBeta[i].reshape(layer[i+1],layer[i]+1),)\n",
    "    return reshapedBeta\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forwardPropagation(flatBeta, layer, flatX, sampleSize):\n",
    "    '''Forward Propagation is the hypothesis function for Neural Networks'''\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "#     Y_0 (5000, 400)\n",
    "    Y = flatX.reshape(sampleSize, -1)\n",
    "#     Z_Y = ()\n",
    "    Y_byLayer = ()\n",
    "    for beta in betaSet:\n",
    "#         Z_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        Z = np.dot(np.insert(Y, 0, 1, axis=1), beta.T)\n",
    "#         Y_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        Y = sigmoid(Z)\n",
    "#         Z_Y += ((Z, Y),)\n",
    "        Y_byLayer += (Y,)\n",
    "#     Y_2 (5000, 10)\n",
    "    return Y_byLayer\n",
    "\n",
    "def sigmoidGradient(Z):\n",
    "    return sigmoid(Z)*(1-sigmoid(Z))\n",
    "\n",
    "\"\"\"this function has to be revised;\n",
    "there should be a way to get cost function using matrix operations without for-loop\"\"\"\n",
    "# def costFunction(beta, X, y, iLambda = 0.):\n",
    "#     sampleSize, numVariables = X.shape  # X(5000, 401)\n",
    "#     Y = np.array([np.unique(y)]* y.shape[0]) == y\n",
    "# #     hypothesis matrix H(5000, 10)\n",
    "#     H = forwardPropagation(betaSet, X)\n",
    "# #     cost function matrix J (5000, 5000) = Y.T(5000, 10)*H(10, 5000)\n",
    "#     J = (- np.dot(Y, np.log(H).T) - np.dot((1-Y), np.log(1-H).T))/sampleSize\n",
    "# #     regularisation term (R)\n",
    "#     cummulativeR = 0\n",
    "#     for beta in betaSet:\n",
    "#         cummulativeR += np.sum(beta*beta) # element-wise multiplication\n",
    "#     cummulativeR *= iLambda/(2*sampleSize)\n",
    "#     return J/sampleSize + cummulativeR\n",
    "\n",
    "def costFunction(flatBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    sampleSize, numVariables = X.shape  # X(1, 401)\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "    J = 0\n",
    "    for n in range(sampleSize):\n",
    "        x_n = X[n:n+1,:]\n",
    "        y_n = Y[n:n+1,:]\n",
    "#         hypothesis vector h_n(1, 10)\n",
    "        h_n = forwardPropagation(flatBeta, layer, x_n, 1)[len(betaSet)-1]\n",
    "#         cost function scalar j_n(1, 1) = y_n(1, 10)*h_n.T(10, 1)\n",
    "        j_n = (- np.dot(y_n, np.log(h_n).T) - np.dot((1-y_n), np.log(1-h_n).T))\n",
    "        J += j_n\n",
    "#     regularisation term (R)\n",
    "    cummulativeR = 0\n",
    "    for beta in betaSet:\n",
    "        cummulativeR += np.sum(beta*beta) #element-wise multiplication\n",
    "    cummulativeR *= iLambda/(2*sampleSize)\n",
    "    return J[0][0]/sampleSize + cummulativeR\n",
    "\n",
    "# def gradient(beta, X, y, iLambda=0.):\n",
    "#     sampleSize, numVariables = X.shape\n",
    "# #     hypothesis (h)\n",
    "#     h = hypothesis(beta, X)\n",
    "# #     error vector e(5000x1) = h(5000x1) - y(5000x1)\n",
    "#     e = h - y\n",
    "# #     gradient vector g(401x1) = e.T(1x5000)*X(401x5000)\n",
    "#     g = np.dot(X.T,e)/(sampleSize)\n",
    "# #     regularisation term vector (r(400x1)) — derivative of the regularisation term of the cost funtion\n",
    "#     r = beta[1:, None]*(iLambda/sampleSize)\n",
    "#     g[1:] = g[1:] + r\n",
    "#     return g.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Neural Network Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "layer = 400, 25, 10\n",
    "X, y = getData(pathToDataFile)\n",
    "sampleSize, numVariables = X.shape\n",
    "flatX = X.flatten()\n",
    "yUnique = np.unique(y)\n",
    "betaTest = flattenBeta((weights['Theta1'], weights['Theta2']))\n",
    "betaInitial = flattenBeta(generateBeta(layer))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "for beta in generateBeta(layer): print(beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Forward-Propagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "print(Y[0:0+1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "print(forwardPropagation(betaTest, layer, flatX, sampleSize)[1].shape)\n",
    "print(forwardPropagation(betaTest, layer, X[0:0+1,:], 1)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(1, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28762916516131876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0][None,:].shape)\n",
    "# costFunction(betaTest, layer, X.flatten(), sampleSize, y, yUnique, iLambda = 0.)\n",
    "costFunction(betaTest, layer, X[0:100][None,:].flatten(), sampleSize, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cost-Function Test\n",
    "The outputs of the costFunction should be as follows:<br\\>\n",
    "betaTest, X, iLambda=0. — 0.287629165161<br\\>\n",
    "betaTest, X, iLambda=1. — 0.384487796243<br\\>\n",
    "betaTest, X, iLambda=0. — 0.287629165161 — weird result<br\\>\n",
    "betaInitial, X, iLambda=1. — 65.5961451562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n",
      "0.384487796243\n",
      "0.287629165161\n",
      "59.6276430654\n"
     ]
    }
   ],
   "source": [
    "print(costFunction(betaTest, layer, flatX, sampleSize, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(betaTest, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))\n",
    "print(costFunction(betaTest, layer, X[0][None,:].flatten(), sampleSize, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(betaInitial, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,9) (1,25) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-54174ddffbe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#         print(l)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         print(h_n[l].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0me_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_n\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigmoidGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetaInitial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,9) (1,25) "
     ]
    }
   ],
   "source": [
    "sampleSize, numVariables = X.shape\n",
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "# print (Y.shape)\n",
    "\n",
    "nLayer = len(layer)-1\n",
    "deltaSet = ()\n",
    "inLayer = X.shape[1]\n",
    "for i in range(len(layer)-1):\n",
    "    deltaSet += (np.zeros((layer[i+1], layer[i]+1)),)\n",
    "# print (deltaSet[1].shape)\n",
    "\n",
    "for n in range(sampleSize):#range(sampleSize)\n",
    "#         x_n(1, 400)\n",
    "    x_n = X[n:n+1,:]\n",
    "#     print(x_n.shape)\n",
    "#         y_n(1, 10)\n",
    "    y_n = Y[n:n+1,:]\n",
    "#     print(y_n.shape)\n",
    "#         h_n(1, 10)\n",
    "    h_n = forwardPropagation(betaInitial, layer, x_n, 1)\n",
    "#     print(h_n.shape)\n",
    "#         error e_n(1, 10) for a given sample (x_n, y_n)\n",
    "    e_n = h_n[nLayer-1] - y_n\n",
    "#     print(e_n.shape)\n",
    "    for l in reversed(range(nLayer)):\n",
    "#         print(l)\n",
    "#         print(h_n[l].shape)\n",
    "        e_n = np.dot(e_n*sigmoidGradient(h_n[l]), betaInitial[l])[:,1:]\n",
    "        np.dot(h_n[l].T, np.insert(e_n, 0, 1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backPropagation(flatBeta, layer, flatX, sampleSize, y, yUnique):\n",
    "    sampleSize, numVariables = X.shape\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "\n",
    "    deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "    H = forwardPropagation(flatBeta, layer, flatX, sampleSize)\n",
    "#     error matrix E(5000, 10)\n",
    "    E = H[len(layer)-2] - Y\n",
    "    for l in reversed(range(len(layer)-1)):\n",
    "        E = np.dot(E*sigmoidGradient(H[l]), betaSet[l])[:,1:]\n",
    "        deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "    flatDelta = flattenBeta(deltaSet)\n",
    "    return flatBeta + flatDelta/sampleSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14421.6429741\n",
      "170.671193875\n"
     ]
    }
   ],
   "source": [
    "betaInitial = flattenBeta(generateBeta(layer))\n",
    "a = backPropagation(betaInitial, layer, flatX, sampleSize, y, yUnique)\n",
    "\n",
    "print(np.sum(a))\n",
    "print(costFunction(a,layer, flatX, sampleSize, y, yUnique, iLambda = 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.93098878501e-08\n",
      "4.58300064565e-09\n",
      "5.59907675779e-08\n",
      "7.16937620382e-08\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.20195764453e-05\n",
      "0.0\n",
      "9.32232069317e-08\n"
     ]
    }
   ],
   "source": [
    "def gradientCheck(beta, layer, sampleSize, epsilon):\n",
    "    for i in np.random.randint(beta.size, size=10):\n",
    "        epsilonVector = np.zeros(beta.size)\n",
    "        epsilonVector[i] = epsilon\n",
    "        betaPlus = betaMinus = beta\n",
    "        betaPlus = betaPlus + epsilonVector\n",
    "        costPlus = costFunction(betaMinus,layer, X, sampleSize, y, yUnique, iLambda = 0.)\n",
    "        betaMinus = betaMinus - epsilonVector\n",
    "        costMinus = costFunction(betaMinus,layer, X, sampleSize, y, yUnique, iLambda = 0.)\n",
    "        approximateGradient = (costPlus-costMinus)/(2*epsilon)\n",
    "        print (approximateGradient)\n",
    "\n",
    "epsilon = 0.0001\n",
    "gradientCheck(betaInitial, layer, sampleSize, epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomBetaSet = generateBeta(layer)\n",
    "# initialBeta = flattenBeta(betaSet)\n",
    "# iLambda = 0\n",
    "# a = optimize.fmin_cg(costFunction, x0=initialBeta,\n",
    "#                        fprime=backPropagation, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                        maxiter=50,disp=True,full_output=True)\n",
    "# len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_1(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "    optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                      method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "#     optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "#                                      args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                      maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_2(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "#     optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                       method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "    optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "                                     args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                     maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = betaOptimisation_1(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = betaOptimisation_2(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qualityControl(optimisedBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    X = flatX.reshape(sampleSize,-1)\n",
    "    yAssignmentVector = []\n",
    "    misAssignedIndex = []\n",
    "    for n in range(sampleSize):\n",
    "        x = X[n]\n",
    "        yAssignment =  np.argmax(forwardPropagation(optimisedBeta, layer, X[n], 1)[1]) + 1\n",
    "        if yAssignment == y[n]:\n",
    "            yAssignmentVector += [True]\n",
    "        else:\n",
    "            yAssignmentVector += [False]\n",
    "            misAssignedIndex += [n]\n",
    "    return (sum(yAssignmentVector)/sampleSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neuralNetworkClassifier(, flatX, sampleSize, y, yUnique, iLambda=0.)\n",
    "qualityControl(a['x'], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qualityControl(b[0], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
