{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.image import NonUniformImage\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "pathToDataFile = 'ex4data1.mat'\n",
    "data = loadmat(pathToDataFile)\n",
    "pathToWeightsFile = 'ex4weights.mat'\n",
    "weights = loadmat(pathToWeightsFile)\n",
    "print(weights['Theta1'].shape)\n",
    "print(weights['Theta2'].shape)\n",
    "print(type(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData(pathToDataFile):\n",
    "    data = loadmat(pathToDataFile)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "def flattenX(X):\n",
    "    flatX = X.flatten()\n",
    "    return flatX\n",
    "\n",
    "def reshapeX(flatX, sampleSize):\n",
    "    reshapedX = flatX.reshape(sampleSize,-1)\n",
    "    return reshapedX\n",
    "\n",
    "def generateBeta(layer):\n",
    "    randomBetaSet = ()\n",
    "    for i in range(len(layer)-1):\n",
    "#         recommendation from Andrew Ng window is ±(6/(inLayer + outLayer))**0.5\n",
    "        low, high = -(6/(layer[i]+layer[i+1]))**0.5, (6/(layer[i]+layer[i+1]))**0.\n",
    "        randomBetaSet += (np.random.uniform(low,high,(layer[i+1], layer[i]+1)),)\n",
    "#         betaSet += (np.zeros((outLayer, inLayer+1)),)\n",
    "    return randomBetaSet\n",
    "\n",
    "def flattenBeta(betaSet):\n",
    "    flatBeta = betaSet[0].flatten()\n",
    "    for beta in betaSet[1:]:\n",
    "        flatBeta = np.concatenate((flatBeta, beta.flatten()), axis=-1)\n",
    "    return flatBeta\n",
    "\n",
    "def reshapeBeta(flatBeta, layer):\n",
    "    splitIndex = 0\n",
    "    splitIndices = []\n",
    "    for i in range(len(layer)-2):\n",
    "        splitIndex += (layer[i]+1)*layer[i+1]\n",
    "        splitIndices += [splitIndex]\n",
    "    splitBeta = np.split(flatBeta, splitIndices)\n",
    "    reshapedBeta = ()\n",
    "    for i in range(len(splitBeta)):\n",
    "        reshapedBeta += (splitBeta[i].reshape(layer[i+1],layer[i]+1),)\n",
    "    return reshapedBeta\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forwardPropagation(flatBeta, layer, flatX, sampleSize):\n",
    "    \"\"\"Forward Propagation is the hypothesis function for Neural Networks\"\"\"\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "#     a_0 (5000, 400)\n",
    "    a = reshapeX(flatX, sampleSize)\n",
    "#     z_a = ()\n",
    "    a_byLayer = ()\n",
    "    for beta in betaSet:\n",
    "#         z_l (5000, j_l); l is the number of layers [0, ...,L]; j is the number of neurons in a layer l [1,...,J]\n",
    "        z = np.dot(np.insert(a, 0, 1, axis=1), beta.T)\n",
    "#         a_l (5000, j_l); l is the number of layers [0, ...,L]; j is the number of neurons in a layer l [1,...,J]\n",
    "        a = sigmoid(z)\n",
    "#         z_a += ((z, a),)\n",
    "        a_byLayer += (a,)\n",
    "#     a_2 (5000, 10)\n",
    "    return a_byLayer\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\"\"\"this function has to be revised;\n",
    "there should be a way to get cost function using matrix operations without for-loop\"\"\"\n",
    "# def costFunction(beta, X, y, iLambda = 0.):\n",
    "#     sampleSize, numVariables = X.shape  # X(5000, 401)\n",
    "#     Y = np.array([np.unique(y)]* y.shape[0]) == y\n",
    "# #     hypothesis matrix H(5000, 10)\n",
    "#     H = forwardPropagation(betaSet, X)\n",
    "# #     cost function matrix J (5000, 5000) = Y.T(5000, 10)*H(10, 5000)\n",
    "#     J = (- np.dot(Y, np.log(H).T) - np.dot((1-Y), np.log(1-H).T))/sampleSize\n",
    "# #     regularisation term (R)\n",
    "#     cummulativeR = 0\n",
    "#     for beta in betaSet:\n",
    "#         cummulativeR += np.sum(beta*beta) # element-wise multiplication\n",
    "#     cummulativeR *= iLambda/(2*sampleSize)\n",
    "#     return J/sampleSize + cummulativeR\n",
    "\n",
    "def costFunction(flatBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    sampleSize, numVariables = X.shape  # X(1, 401)\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "    J = 0\n",
    "    for n in range(sampleSize):\n",
    "        x_n = X[n:n+1,:]\n",
    "        y_n = Y[n:n+1,:]\n",
    "#         hypothesis vector h_n(1, 10)\n",
    "        h_n = forwardPropagation(flatBeta, layer, x_n, 1)[len(betaSet)-1]\n",
    "#         cost function scalar j_n(1, 1) = y_n(1, 10)*h_n.T(10, 1)\n",
    "        j_n = (- np.dot(y_n, np.log(h_n).T) - np.dot((1-y_n), np.log(1-h_n).T))\n",
    "        J += j_n\n",
    "#     regularisation term (R)\n",
    "    cummulativeR = 0\n",
    "    for beta in betaSet:\n",
    "        cummulativeR += np.sum(beta*beta) #element-wise multiplication\n",
    "    cummulativeR *= iLambda/(2*sampleSize)\n",
    "    return J[0][0]/sampleSize + cummulativeR\n",
    "\n",
    "# def gradient(beta, X, y, iLambda=0.):\n",
    "#     sampleSize, numVariables = X.shape\n",
    "# #     hypothesis (h)\n",
    "#     h = hypothesis(beta, X)\n",
    "# #     error vector e(5000x1) = h(5000x1) - y(5000x1)\n",
    "#     e = h - y\n",
    "# #     gradient vector g(401x1) = e.T(1x5000)*X(401x5000)\n",
    "#     g = np.dot(X.T,e)/(sampleSize)\n",
    "# #     regularisation term vector (r(400x1)) — derivative of the regularisation term of the cost funtion\n",
    "#     r = beta[1:, None]*(iLambda/sampleSize)\n",
    "#     g[1:] = g[1:] + r\n",
    "#     return g.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = getData(pathToDataFile)\n",
    "sampleSize, numVariables = X.shape\n",
    "flatX = flattenX(X)\n",
    "layer = 400, 25, 10\n",
    "yUnique = np.unique(y)\n",
    "betaSet = (weights['Theta1'], weights['Theta2'])\n",
    "flatBeta = flattenBeta(betaSet)\n",
    "betaSet_zero = generateBeta(layer)\n",
    "flatBeta_zero = flattenBeta(betaSet_zero)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "Y[0:0+1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardPropagation(flatBeta, layer, flatX, sampleSize)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardPropagation(flatBeta, layer, X[0:0+1,:], 1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n",
      "0.287629165161\n",
      "0.384487796243\n",
      "64.3493922545\n"
     ]
    }
   ],
   "source": [
    "print(costFunction(flatBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(flatBeta, layer, X[0][None,:], sampleSize, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(flatBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))\n",
    "print(costFunction(flatBeta_zero, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleSize, numVariables = X.shape\n",
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "# print (Y.shape)\n",
    "\n",
    "nLayer = len(layer)-1\n",
    "deltaSet = ()\n",
    "inLayer = X.shape[1]\n",
    "for i in range(len(layer)-1):\n",
    "    deltaSet += (np.zeros((layer[i+1], layer[i]+1)),)\n",
    "# print (deltaSet[1].shape)\n",
    "\n",
    "for n in range(sampleSize):#range(sampleSize)\n",
    "#         x_n(1, 400)\n",
    "    x_n = X[n:n+1,:]\n",
    "#     print(x_n.shape)\n",
    "#         y_n(1, 10)\n",
    "    y_n = Y[n:n+1,:]\n",
    "#     print(y_n.shape)\n",
    "#         h_n(1, 10)\n",
    "    h_n = forwardPropagation(flatBeta, layer, x_n, 1)\n",
    "#     print(h_n.shape)\n",
    "#         error e_n(1, 10) for a given sample (x_n, y_n)\n",
    "    e_n = h_n[nLayer-1] - y_n\n",
    "#     print(e_n.shape)\n",
    "    for l in reversed(range(nLayer)):\n",
    "#         print(l)\n",
    "#         print(h_n[l].shape)\n",
    "        e_n = np.dot(e_n*sigmoidGradient(h_n[l]), betaSet[l])[:,1:]\n",
    "        np.dot(h_n[l].T, np.insert(e_n, 0, 1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backPropagation(flatBeta, layer, flatX, sampleSize, y, yUnique):\n",
    "    sampleSize, numVariables = X.shape\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "\n",
    "    deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "    H = forwardPropagation(flatBeta, layer, flatX, sampleSize)\n",
    "#     error matrix E(5000, 10)\n",
    "    E = H[len(layer)-2] - Y\n",
    "    for l in reversed(range(len(layer)-1)):\n",
    "        E = np.dot(E*sigmoidGradient(H[l]), betaSet[l])[:,1:]\n",
    "        deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "    flatDelta = flattenBeta(deltaSet)\n",
    "    return flatBeta + flatDelta/sampleSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16554.0188782\n",
      "202.579344362\n"
     ]
    }
   ],
   "source": [
    "randomBetaSet = generateBeta(layer)\n",
    "flatRandomBetaSet = flattenBeta(randomBetaSet)\n",
    "a = backPropagation(flatRandomBetaSet, layer, flatX, sampleSize, y, yUnique)\n",
    "\n",
    "print(np.sum(a))\n",
    "print(costFunction(a,layer, flatX, sampleSize, y, yUnique, iLambda = 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradientCheck(flatBeta, layer, epsilon):\n",
    "    for i in np.random.randint(flatBeta.size, size=10):\n",
    "        epsilonVector = np.zeros(flatBeta.size)\n",
    "        epsilonVector[i] = epsilon\n",
    "        betaPlus = betaMinus = flatBeta\n",
    "        betaPlus = betaPlus + epsilonVector\n",
    "        costPlus = costFunction(betaPlus,layer, X, y, yUnique, iLambda = 0.)\n",
    "        betaMinus = betaMinus - epsilonVector\n",
    "        costMinus = costFunction(betaMinus,layer, X, y, yUnique, iLambda = 0.)\n",
    "        approximateGradient = (costPlus-costMinus)/(2*epsilon)\n",
    "#         print (approximateGradient)\n",
    "\n",
    "# gradientCheck(flatBeta, layer, 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomBetaSet = generateBeta(layer)\n",
    "# initialBeta = flattenBeta(betaSet)\n",
    "# iLambda = 0\n",
    "# a = optimize.fmin_cg(costFunction, x0=initialBeta,\n",
    "#                        fprime=backPropagation, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                        maxiter=50,disp=True,full_output=True)\n",
    "# len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_1(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "    optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                      method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "#     optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "#                                      args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                      maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_2(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "#     optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                       method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "    optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "                                     args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                     maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = betaOptimisation_1(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.287629\n",
      "         Iterations: 0\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 98\n"
     ]
    }
   ],
   "source": [
    "b = betaOptimisation_2(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qualityControl(optimisedBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    X = reshapeX(flatX, sampleSize)\n",
    "    yAssignmentVector = []\n",
    "    misAssignedIndex = []\n",
    "    for n in range(sampleSize):\n",
    "        x = X[n]\n",
    "        yAssignment =  np.argmax(forwardPropagation(optimisedBeta, layer, X[n], 1)[1]) + 1\n",
    "        if yAssignment == y[n]:\n",
    "            yAssignmentVector += [True]\n",
    "        else:\n",
    "            yAssignmentVector += [False]\n",
    "            misAssignedIndex += [n]\n",
    "    return (sum(yAssignmentVector)/sampleSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neuralNetworkClassifier(, flatX, sampleSize, y, yUnique, iLambda=0.)\n",
    "qualityControl(a['x'], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualityControl(b[0], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
