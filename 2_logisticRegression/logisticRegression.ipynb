{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import csv\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Univariate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFile_1 = 'ex2data1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dF_1 = pd.read_csv(dataFile_1, header = None, names = ['x1', 'x2', 'y'])\n",
    "sampleSize_1, nVariables_1 = dF_1.shape\n",
    "print(dF_1.head())\n",
    "print (\"sampleSize =\", sampleSize_1, \"nVariables =\", nVariables_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = dF_1['x1']\n",
    "x2 = dF_1['x2']\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x1, x2, c = dF_1['y'])\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Extraction and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(dataFile):\n",
    "#     try wiht matrices as well\n",
    "    data = np.loadtxt(dataFile, delimiter = ',')\n",
    "    sampleSize, nVariables = data.shape\n",
    "    X = np.insert(data[:, :-1], 0, 1, axis=1)\n",
    "    y = data[:, -1:]\n",
    "#     beta = np.matrix(np.zeros(nVariables)).T\n",
    "    beta = np.zeros(nVariables)\n",
    "    return beta, X.flatten(), y.flatten(), sampleSize, nVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Logistic Regression\n",
    "### 1.3.1 Logistic Regression\n",
    "\n",
    "**Sigmoid Function** ${\\sigma}(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "\n",
    "### 1.3.2 Vectorisation of Logistic Regression\n",
    "\n",
    "**Hypothesis** $h_{\\beta}(X) =   \\frac{1}{1 + e^{X\\cdot\\beta}}$\n",
    "\n",
    "**Cost Function** $J = \\frac{-1}{n}\\sum(y^T\\cdot \\log h_{\\beta} +(1-y)^T\\cdot \\log (1-h_{\\beta}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def hypothesis(beta, X, sampleSize, nVariables):\n",
    "    beta = beta.reshape(nVariables, -1)\n",
    "    X = X.reshape(sampleSize, -1)\n",
    "    return sigmoid(np.dot(X, beta))\n",
    "\n",
    "# def costFunction(X, y, beta):\n",
    "def costFunction(beta, X, y, sampleSize, nVariables, iLambda = 0.):\n",
    "#     beta = beta.reshape(nVariables, -1)\n",
    "#     X = X.reshape(sampleSize, -1)\n",
    "    y = y.reshape(sampleSize, -1)\n",
    "#     hypothesis vector h(n, 1)\n",
    "    h = hypothesis(beta, X, sampleSize, nVariables)\n",
    "#     cost scalar J(1, 1)\n",
    "    J = (- np.dot(y.T, np.log(h)) - np.dot((1-y).T, np.log(1-h)))/sampleSize\n",
    "#     similarly cost J can be calculated using np.multiply together with np.sum\n",
    "#     cost = -np.sum(np.multiply(y, np.log(h)) + np.multiply((1-y), np.log(1-h)))/sampleSize\n",
    "#     regularisation scalar (R)\n",
    "    R = iLambda*np.dot(beta[1:].T,beta[1:])/(2*sampleSize)\n",
    "    return (J + R)[0][0]\n",
    "\n",
    "def betaOptimisation_1 (beta, X, y, sampleSize, nVariables, iLambda=0.):\n",
    "    return optimize.fmin(costFunction, x0=beta, args=(X, y, sampleSize, nVariables, iLambda), maxiter=1500, full_output=True)\n",
    "\n",
    "def prediction(beta, X, sampleSize, nVariables):\n",
    "    return hypothesis(beta, X, sampleSize, nVariables) >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Function Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaTest_1, X_1, y_1, sampleSize_1, nVariables_1 = getData(dataFile_1)\n",
    "y_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Cost-Function Test\n",
    "The outputs of the costFunction should be as follows:<br\\>\n",
    "betaTest (set to zeros), X, iLambda=0. — **J = 0.693** (Andrew Ng) <br\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"J =\", costFunction(betaTest_1, X_1, y_1, sampleSize_1, nVariables_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Prediction Test\n",
    "The outputs of the costFunction should be as follows:<br\\>\n",
    "Exam_1: 45, Exam_2: 85 — **P = 0.776** (Andrew Ng) <br\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaOpt_1 = betaOptimisation_1(betaTest_1, X_1, y_1, sampleSize_1, nVariables_1)[0]\n",
    "xTest_1 = np.array([1, 45, 85])\n",
    "sampleSizeTest_1 = 1\n",
    "print(\"P =\", hypothesis(betaOpt_1, xTest_1, sampleSizeTest_1, nVariables_1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Results Visualisation & Analysis\n",
    "### 1.5.1 Goodness of Fit Measures\n",
    "#### 1.5.1.1 Decision Boundary\n",
    "This comment is here thanks to this dude (https://github.com/vsevolodloik).<br />\n",
    "Decision boundary is defined as follows:<br />\n",
    "$\\frac{1}{1 + e^{X\\cdot\\beta}} = \\frac{1}{2}$<br />\n",
    "Therefore, for the simple case of two variables, the equation of decision boundary takes the following form:<br />\n",
    "$\\beta_0+\\beta_1\\cdot{X_1}+\\beta_2 \\cdot{X_2} = 0$\n",
    "#### 1.5.1.2 Types of Errors & Accuracy, Precision, Recal\n",
    "\n",
    "The rate **type I error** (false positives) is denoted by $\\alpha$.<br />\n",
    "The rate **type II error** (false negatives) is denoted by $\\beta$.<br /><br />\n",
    "**Accuracy** $= \\frac {tP + tN}{tP + tN + fP + fN}$<br /><br />\n",
    "**Precision** $= \\frac {tP}{tP + fP}$<br /><br />\n",
    "**Recall** $= \\frac {tP}{tP + fN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def goodnessOfFit(beta, X, y,  sampleSize, nVariables):\n",
    "    beta_R = beta.reshape(nVariables, -1)\n",
    "    X_R = X.reshape(sampleSize, -1)\n",
    "    y_R = y.reshape(sampleSize, -1)\n",
    "    p = prediction(beta, X, sampleSize, nVariables).flatten()\n",
    "    \n",
    "#     Elegant way to calculate tP, fP, and fN\n",
    "    tP = np.sum(y*p)\n",
    "    fP = np.sum(y-p==-1)\n",
    "    fN = np.sum(y-p==1)\n",
    "    precision  = tP/(tP+fP)\n",
    "    recall  = tP/(tP+fN)\n",
    "    accuracy = (X.shape[0] - fP - fN)/X.shape[0]\n",
    "    print(\"Accuracy\", accuracy, \"\\nPrecision =\", precision, \"\\nRecall =\", recall)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    x1 = X_R[:, 1:2]\n",
    "    x2 = X_R[:, 2:]\n",
    "    plt.scatter(x1, x2, c = y_R[:, 0:])\n",
    "    x2Fit = - beta_R[0]/beta_R[2] - x1*beta_R[1]/beta_R[2]\n",
    "    plt.plot(x1, x2Fit, '-')\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goodnessOfFit(betaOpt_1, X_1, y_1, sampleSize_1, nVariables_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.johnwittenauer.net/tag/machine-learning/\n",
    "\n",
    "http://aimotion.blogspot.se/2011/11/machine-learning-with-python-logistic.html\n",
    "\n",
    "https://beckernick.github.io/logistic-regression-from-scratch/\n",
    "\n",
    "https://github.com/kaleko/CourseraML/blob/master/ex2/ex2.ipynb\n",
    "\n",
    "http://www.scipy-lectures.org/advanced/mathematical_optimization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 Multivariate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFile_2 = 'ex2data2.txt'\n",
    "dF_2 = pd.read_csv(dataFile_2, header = None)\n",
    "sampleSize, nVariables = dF_2.shape\n",
    "print (\"sampleSize =\", sampleSize, \"nVariables =\", nVariables)\n",
    "print (dF_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_1s = dF_2.iloc[:, :1]\n",
    "X_2s = dF_2.iloc[:, 1:2]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_1s, X_2s, c = dF_2.iloc[:, 2:])\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Extraction Transformation\n",
    "Add **polynomial** and **interaction** features using **SciKitLearn Preprocessing**<br\\>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPolynomial(dataFile, polynomialDegree):\n",
    "    data = np.loadtxt(dataFile, delimiter = ',')\n",
    "    sampleSize, nVariables = data.shape\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1:]\n",
    "    poly = PolynomialFeatures(polynomialDegree)\n",
    "#     X without intercept is passed to PolynomialFeatures.fit_transform.\n",
    "#     Intercept is added automatically.\n",
    "    polyX = poly.fit_transform(X)\n",
    "    sampleSize, nVariables = polyX.shape\n",
    "    beta = np.zeros((nVariables,1))\n",
    "    return beta.flatten(), polyX.flatten(), y.flatten(), sampleSize, nVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Function Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betaPoly6, XPoly6, yPoly6, sampleSizePoly6, nVariablesPoly6 = addPolynomial(dataFile_2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Cost-Function Test\n",
    "The outputs of the costFunction should be as follows:<br\\>\n",
    "betaTest (set to zeros), X, iLambda=0. — **J = 0.693** (Andrew Ng) <br\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"J =\",costFunction(betaPoly6, XPoly6, yPoly6, sampleSizePoly6, nVariablesPoly6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_2(beta, X, y, sampleSize, nVariables, iLambda=0.):\n",
    "\n",
    "    optimisedBeta = optimize.minimize(costFunction, beta, args=(X, y, sampleSize, nVariables, iLambda),\n",
    "                                      method='BFGS', options={'maxiter':50})\n",
    "\n",
    "#     optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "#                                      args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                      maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# betaOpt = betaOptimisation(betaPoly6, XPoly6, yPoly6, iLambda = 0.)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Results Visualisation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decisionBoundary(beta, X, y, sampleSize, nVariables, xMin, xMax, step, polyOrder, iLambda=0.):\n",
    "    p = prediction(beta, X, sampleSize, nVariables).flatten()\n",
    "    tP = np.sum(y*p)\n",
    "    fP = np.sum(y-p==-1)\n",
    "    fN = np.sum(y-p==1)\n",
    "    precision  = tP/(tP+fP)\n",
    "    recall  = tP/(tP+fN)\n",
    "    accuracy = (X.shape[0] - fP - fN)/X.shape[0]\n",
    "    print(\"Accuracy\", accuracy, \"\\nPrecision =\", precision, \"\\nRecall =\", recall)\n",
    "    \n",
    "    x1 = np.linspace(xMin[0], xMax[0], step)\n",
    "    x2 = np.linspace(xMin[1], xMax[1], step)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    combinedX = np.concatenate((X1.reshape(step**2, -1), X2.reshape(step**2, -1)), axis=1)\n",
    "    # X without intercept is passed to PolynomialFeatures.fit_transform.\n",
    "    # Intercept is added automatically.\n",
    "    poly = PolynomialFeatures(polyOrder)\n",
    "    polyX = poly.fit_transform(combinedX)\n",
    "    Y = hypothesis(beta, polyX, step**2, polyX.shape[1])\n",
    "    # Y = prediction(betaOpt_2, polyX, 2500, nVariablesPoly6)\n",
    "    Y.reshape(step, -1)\n",
    "    \n",
    "#     plt.figure(figsize=(8,6))\n",
    "    decisionBoundary = plt.contour(X1, X2, Y.reshape(step, -1), [0.5])\n",
    "#     label = {0:'Lambda = %d'%iLambda}\n",
    "    plt.clabel(decisionBoundary, inline=1, fontsize=10, fmt = '$\\lambda $= %d'%iLambda)\n",
    "\n",
    "    plt.title(\"Decision Boundary\")\n",
    "\n",
    "    x1s = X.reshape(sampleSize,-1)[:, 1:2]\n",
    "    x2s = X.reshape(sampleSize,-1)[:, 2:3]\n",
    "    plt.scatter(x1s, x2s, c = y.reshape(sampleSize,-1)[:, 0:])\n",
    "\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iLambda = 0\n",
    "polyOrder = 6\n",
    "betaPoly, XPoly, yPoly, sS_Poly, nV_Poly = addPolynomial(dataFile_2, polyOrder)\n",
    "betaOpt_2 = betaOptimisation_2(betaPoly, XPoly, yPoly, sS_Poly, nV_Poly, iLambda)\n",
    "\n",
    "xMin = (-1., -1.)\n",
    "xMax = (1.2, 1.2)\n",
    "step = 50\n",
    "decisionBoundary(betaOpt_2, XPoly, yPoly, sS_Poly, nV_Poly, xMin, xMax, step, polyOrder, iLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, iLambda in enumerate([0., 1., 10, 100 ]):\n",
    "    polyOrder = 6\n",
    "    betaPoly, XPoly, yPoly, sampleSizePoly, nVariablesPoly = addPolynomial(dataFile_2, polyOrder)\n",
    "    betaOpt = betaOptimisation_2(betaPoly, XPoly, yPoly, sampleSizePoly, nVariablesPoly, iLambda)\n",
    "    xMin = (-1., -1.)\n",
    "    xMax = (1.2, 1.2)\n",
    "    step = 50\n",
    "    decisionBoundary(betaOpt, XPoly, yPoly, sampleSizePoly, nVariablesPoly, xMin, xMax, step, polyOrder, iLambda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
