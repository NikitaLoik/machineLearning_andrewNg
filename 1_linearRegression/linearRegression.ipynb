{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathToDataFile = 'ex1data1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleSize = 97 numVariables = 2\n",
      "        0        1\n",
      "0  6.1101  17.5920\n",
      "1  5.5277   9.1302\n",
      "2  8.5186  13.6620\n",
      "3  7.0032  11.8540\n",
      "4  5.8598   6.8233\n"
     ]
    }
   ],
   "source": [
    "dataFrame = pd.read_csv(pathToDataFile, header = None)\n",
    "sampleSize, numVariables = dataFrame.shape\n",
    "print (\"sampleSize =\", sampleSize, \"numVariables =\", numVariables)\n",
    "print (dataFrame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAENCAYAAADgwHn9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1166c6128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xs = dataFrame.iloc[:, :1]\n",
    "Ys = dataFrame.iloc[:, 1:2]\n",
    "plt.plot(Xs, Ys, 'o')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(pathToDataFile):\n",
    "    data = np.loadtxt(pathToDataFile, delimiter = ',')\n",
    "    sampleSize, numVariables = data.shape\n",
    "#     matrix of variables X(n, k), where n is sampleSize, and k is numVariables, including the intercept\n",
    "    X = np.insert(data[:, :-1], 0, 1, axis=1)\n",
    "#     vector of response y(n, 1), where n is sampleSize\n",
    "    y = data[:, -1:]\n",
    "#     vector of coefficients beta(k, 1), where k is numVariables, including the intercept\n",
    "    beta = np.zeros((numVariables,1))\n",
    "    return beta, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta, X, y = getData(pathToDataFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Computing Parameters Analytically\n",
    "**Coefficient Matrix** can be calculated as follows \n",
    "$$\\beta = (X^T\\cdot X)^{-1}\\cdot X^T\\cdot y$$\n",
    "\n",
    "**X** is a matrix of variables with a first column of dummy variables as intercept (column of ones).\n",
    "**y** is a vector of responses.\n",
    "$\\theta$ is a vector of coefficients in the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing Parameters Analytically\n",
    "def linearRegression_analytical(X, y):\n",
    "#     beta vector (2, 1), vector of coefficients\n",
    "    beta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Computing Parameters Using Gradient Descent & Linear Algebra\n",
    "\n",
    "### Vector Representation of the Regression Problem\n",
    "Variables matrix X:\n",
    "\\begin{equation*}\n",
    "X =\n",
    "\\begin{vmatrix}\n",
    "x_{1,1} & \\ldots & x_{1,j} \\\\\n",
    "\\ldots & \\ldots & \\ldots \\\\\n",
    "x_{n,1} & \\ldots & x_{n,j}\\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Variables marix X, after adding a column of ones as intercepts:\n",
    "\\begin{equation*}\n",
    "X =\n",
    "\\begin{vmatrix}\n",
    "1 & x_{1,1} & \\ldots & x_{1,k} \\\\\n",
    "1 & \\ldots & \\ldots & \\ldots \\\\\n",
    "1 & x_{n,1}& \\ldots & x_{n,k}\\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Response vector Y:\n",
    "\\begin{equation*}\n",
    "Y =\n",
    "\\begin{vmatrix}\n",
    "y_1\\\\\n",
    "\\ldots\\\\\n",
    "y_n\\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Coefficients vector $\\beta$:\n",
    "\\begin{equation*}\n",
    "\\beta =\n",
    "\\begin{vmatrix}\n",
    "\\beta_1\\\\\n",
    "\\ldots\\\\\n",
    "\\beta_k\\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "**Hypothesis** $H_{\\beta}(X) =  X\\cdot\\beta$\n",
    "\n",
    "**Error** $E = (H_{\\beta}(X) - Y)$\n",
    "\n",
    "**Cost Function** $J = \\frac{\\sum(H_{\\beta} - Y)^{2}}{2N}$\n",
    "\n",
    "**Gradient** $G = X^{T}\\cdot E$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypothesis(beta, X):\n",
    "#     return hypothesis vector h(n, 1), where n is sampleSize\n",
    "    return np.dot(X, beta)\n",
    "\n",
    "def costFunction(beta, X, y):\n",
    "    sampleSize, numVariables = X.shape\n",
    "#     hypothesis vector h(n, 1)\n",
    "    h = hypothesis(beta, X)\n",
    "#     cost scalar J(1, 1) = (y-h).T, y-h)/(2*n), where n is sampleSize\n",
    "    J = np.sum((y-h)**2)/(2*sampleSize)\n",
    "#     similarly cost J can be calculated using dot-product\n",
    "#     J(1, 1) = (y-h).T, y-h)/(2*n), where n is sampleSize\n",
    "#     technically the result is an array (1,1) rather than a float\n",
    "#     J = np.dot((y-h).T, y-h)/(2*sampleSize)\n",
    "    return J\n",
    "\n",
    "def gradientDescent(beta, X, y, alpha, iterations):\n",
    "    sampleSize, numVariables = X.shape\n",
    "    J_history = []\n",
    "    for i in range(iterations):\n",
    "#         hypothesis vector h(n, 1)\n",
    "        h = hypothesis(beta, X)\n",
    "#         error vector e(n, 1)\n",
    "        e = h - y\n",
    "#         cost scalar J\n",
    "        J = costFunction(beta, X, y)\n",
    "#         gradient vector g(k, 1)\n",
    "        g = np.dot(X.T, e)\n",
    "#         updated beta vector beta(k, 1)\n",
    "        beta = beta - alpha*g/(sampleSize)\n",
    "#         updated J_history\n",
    "        J_history += [J] \n",
    "    return beta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Results Visulisation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotRegression(beta, X, y):  \n",
    "#     yFit = betaFit[0][0] + betaFit[1][0]*X[:,1:]\n",
    "    yFit = np.dot(X,betaFit)\n",
    "    \n",
    "    MSE = np.sum((y - yFit)**2)/y.shape[0]\n",
    "    \n",
    "    plt.plot(X[:,1:], y, 'o', X[:,1:], yFit, '-')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "    print (\"β_0:\", betaFit[0][0],\n",
    "           \"\\nβ_1:\", betaFit[1][0],\n",
    "           '\\nRegression: Y =\", '{:10.2f}'.format(betaFit[0][0]), '+', '{:10.2f}'.format(betaFit[1][0]), 'X'\n",
    "           '\\nMSE =','{:10.2f}'.format(MSE))\n",
    "    return plt.show()\n",
    "\n",
    "def plotConvergence(J_history, iterations):\n",
    "    plt.plot(np.arange(1, iterations + 1), J_history, '-')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('J (cost)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Analytical Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaFit = linearRegression_analytical(X, y)\n",
    "\n",
    "plotRegression(betaFit, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Gradient-Descent Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "betaFit = gradientDescent(beta, X, y, alpha, iterations)[0]\n",
    "plotRegression(betaFit, X, y)\n",
    "costVector = gradientDescent(beta, X, y, alpha, iterations)[1]\n",
    "plotConvergence(costVector, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data via pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathToDataFile = 'ex1data1.txt'\n",
    "\n",
    "dataFrame = pd.read_csv(pathToDataFile, header = None, names = ['x', 'y'])\n",
    "nRows, nColumns = dataFrame.shape\n",
    "X = dataFrame['x']\n",
    "Y = dataFrame['y']\n",
    "\n",
    "# plt.plot(X, Y, 'o', ms = 5)\n",
    "# plt.xlabel('City Population (10 000)')\n",
    "# plt.ylabel('Profit ($10 000)')\n",
    "# plt.show()\n",
    "# print('Sample Size:', dataFrame.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing Parameters Analytically\n",
    "\n",
    "variablesMatrix = dataFrame.loc[:, 'x':'x']\n",
    "variablesMatrix.insert(0, 'dummy_1', 1)\n",
    "responseVector = dataFrame.loc[:, 'y':'y']\n",
    "#print (variablesMatrix)\n",
    "print (type(variablesMatrix))\n",
    "print (variablesMatrix.shape)\n",
    "\n",
    "coefficientMatrix = np.dot(np.dot(np.linalg.inv(np.dot(variablesMatrix.T, variablesMatrix)), variablesMatrix.T), responseVector)\n",
    "print (coefficientMatrix)\n",
    "\n",
    "fitX = np.arange(5.0, 22.5, 0.5,)\n",
    "fitY = coefficientMatrix[0] + coefficientMatrix[1]*X\n",
    "\n",
    "plt.plot(X, Y, 'o', X, fitY, '-')\n",
    "plt.xlabel('City Population (10 000)')\n",
    "plt.ylabel('Profit ($10 000)')\n",
    "plt.show()\n",
    "\n",
    "print ('θ_0: ', coefficientMatrix[0][0],\n",
    "       '\\nθ_1: ', coefficientMatrix[1][0],\n",
    "      '\\nFit: Y =' '{:10.2f}'.format(coefficientMatrix[0][0]), '+', '{:10.2f}'.format(coefficientMatrix[1][0]), 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://codereview.stackexchange.com/questions/171144/gradient-descent-for-linear-regression-using-numpy-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData (pathToDataFile):\n",
    "    dataFrame = pd.read_csv(pathToDataFile, header = None)\n",
    "    sampleSize, numVariables = dataFrame.shape\n",
    "    variables = dataFrame.iloc[:, :-1]\n",
    "    variables.insert(0, 'intercept', 1)\n",
    "    response = dataFrame.iloc[:, -1:]\n",
    "    coefficients = pd.DataFrame(np.ones((variables.shape[1],1)))\n",
    "    \n",
    "    return variables, response, coefficients\n",
    "\n",
    "def costFunction(variables, response, coefficients):\n",
    "    \"\"\"\n",
    "    costFunciton(variables, response, coefficients) computes the cost of using coefficients as the\n",
    "    parameter for linear regression to fit the data points\n",
    "    \"\"\"\n",
    "    sampleSize, numVariables = variables.shape\n",
    "    costFunction = ((np.sum((response-np.dot(variables, coefficients))**2))/(2*sampleSize))\n",
    "    return costFunction.values[0]\n",
    "\n",
    "def gradientDescent(variables, response, learningRate, iterations):\n",
    "    sampleSize, numVariables = variables.shape\n",
    "    coefficients = pd.DataFrame(np.zeros((numVariables,1)))\n",
    "    costVector = []\n",
    "    for i in range(iterations):\n",
    "        hypothesis = np.dot(variables, coefficients)\n",
    "        error = hypothesis - response\n",
    "        cost = costFunction(variables, response, coefficients)\n",
    "        gradient = np.dot(variables.T, error)\n",
    "        coefficients = coefficients - learningRate*gradient/(sampleSize)\n",
    "        costVector += [cost]\n",
    "    \n",
    "    return coefficients, costVector\n",
    "\n",
    "def plotRegression(variables, response, coefficients):\n",
    "    Xs = variables.iloc[:, 1:]\n",
    "    Ys = response.iloc[:, 0:]\n",
    "    fitYs = coefficients[0][0] + coefficients[0][1]*Xs\n",
    "    plt.plot(Xs, Ys, 'o', Xs, fitYs, '-')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    print ('β_0: ', coefficients[0][0],\n",
    "           '\\nβ_1: ', coefficients[0][1],\n",
    "           '\\nRegression: Y =' '{:10.2f}'.format(coefficients[0][0]), '+', '{:10.2f}'.format(coefficients[0][1]), 'X')\n",
    "    return plt.show()\n",
    "\n",
    "    \n",
    "def plotCostFunctionVSiterations(costVector, iterations):\n",
    "    plt.plot(np.arange(1, iterations + 1), costVector, '-')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cost (J)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathToDataFile = 'ex1data1.txt'\n",
    "\n",
    "iterations = 1500\n",
    "learningRate = 0.01\n",
    "\n",
    "variables, response, coefficients = getData (pathToDataFile)\n",
    "coefficients, costVector = gradientDescent(variables, response, learningRate, iterations)\n",
    "\n",
    "plotRegression(variables, response, coefficients)\n",
    "# for learningRate in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "#     coefficients, costVector = gradientDescent(variables, response, learningRate, iterations)\n",
    "#     print ('Learning Rate =', str(learningRate),'\\nJ =', '{:10.2f}'.format(costVector[-1]))\n",
    "#     plotCostFunctionVSiterations(costVector, iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathToDataFile = 'ex1data2.txt'\n",
    "variables_2, response_2, coefficients_2 = getData (pathToDataFile)\n",
    "\n",
    "gradientDescent(variables_2, response_2, 0.01, 1000)\n",
    "dataFrame_2 = pd.read_csv(pathToDataFile, header = None)\n",
    "scatter_matrix(dataFrame_2, figsize = (12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a' : [4,1,3], 'b' : [5,2,4]},index=[1,2,3])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = pd.Series([0.6,0.4],index=['a','b'])\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
