{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.image import NonUniformImage\n",
    "from matplotlib import cm\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "file_path = '../course_materials/ex4data1.mat'\n",
    "data = loadmat(file_path)\n",
    "eights_file_path = '../course_materials/ex4weights.mat'\n",
    "weights = loadmat(eights_file_path)\n",
    "print(weights['Theta1'].shape)\n",
    "print(weights['Theta2'].shape)\n",
    "print(type(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Nural Network\n",
    "## 1.1 Forward Porpagation\n",
    "\n",
    "<img src=\"../course_materials/forward_propagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    data = loadmat(file_path)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "def get_β(layer):\n",
    "    '''Generate β-matrix for every layer in Neural Network'''\n",
    "    β_set = ()\n",
    "    for i in range(len(layer)-1):\n",
    "#         recommendation from Andrew Ng window is ±(6/(inLayer + outLayer))**0.5\n",
    "        low, high = -(6/(layer[i]+layer[i+1]))**0.5, (6/(layer[i]+layer[i+1]))**0.\n",
    "        β_set += (np.random.uniform(low,high,(layer[i+1], layer[i]+1)),)\n",
    "#         β_set += (np.zeros((outLayer, inLayer+1)),)\n",
    "    return β_set\n",
    "\n",
    "def flatten_β(β_set):\n",
    "    β_flat = β_set[0].flatten()\n",
    "    for β in β_set[1:]:\n",
    "        β_flat = np.concatenate((β_flat, β.flatten()), axis=-1)\n",
    "    return β_flat\n",
    "\n",
    "def reshape_β(β, layer):\n",
    "    splitIndex = 0\n",
    "    splitIndices = []\n",
    "    for i in range(len(layer)-2):\n",
    "        splitIndex += (layer[i]+1)*layer[i+1]\n",
    "        splitIndices += [splitIndex]\n",
    "    splitβ = np.split(β, splitIndices)\n",
    "    reshapedβ = ()\n",
    "    for i in range(len(splitβ)):\n",
    "        reshapedβ += (splitβ[i].reshape(layer[i+1],layer[i]+1),)\n",
    "    return reshapedβ\n",
    "    \n",
    "def get_sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forward_propagation(β_flat, layer, X_flat, n_samples):\n",
    "    '''Forward Propagation is the hypothesis function for Neural Networks'''\n",
    "    β_set = reshape_β(β_flat, layer)\n",
    "#     H_0 (5000, 400)\n",
    "    H = X_flat.reshape(n_samples, -1)\n",
    "#     Z_H = ()\n",
    "    H_byLayer = ()\n",
    "    for β in β_set:\n",
    "#         print(H.shape)\n",
    "#         Z_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        Z = np.dot(np.insert(H, 0, 1, axis=1), β.T)\n",
    "#         H_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        H = get_sigmoid(Z)\n",
    "#         Z_H += ((Z, H),)\n",
    "        H_byLayer += (H,)\n",
    "#     H_2 (5000, 10)\n",
    "    return H_byLayer\n",
    "\n",
    "def get_sigmoid_gradient(Z):\n",
    "    return get_sigmoid(Z)*(1-get_sigmoid(Z))\n",
    "\n",
    "def cost_function(β_flat, layer, X_flat, n_samples, y, yUnique, λ = 0.):\n",
    "    X = X_flat.reshape(n_samples, -1)\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    β_set = reshape_β(β_flat, layer)\n",
    "    J = 0\n",
    "    for n in range(n_samples):\n",
    "        x_n = X[n:n+1,:]\n",
    "        y_n = Y[n:n+1,:]\n",
    "#         hypothesis vector h_n(1, 10)\n",
    "        h_n = forward_propagation(β_flat, layer, x_n, 1)[len(β_set)-1]\n",
    "#         cost function scalar j_n(1, 1) = y_n(1, 10)*h_n.T(10, 1)\n",
    "        j_n = (- np.dot(y_n, np.log(h_n).T) - np.dot((1-y_n), np.log(1-h_n).T))\n",
    "        J += j_n\n",
    "#     regularisation term (R)\n",
    "    cummulativeR = 0\n",
    "    for β in β_set:\n",
    "        cummulativeR += np.sum(β*β) #element-wise multiplication\n",
    "    cummulativeR *= λ/(2*n_samples)\n",
    "    return J[0][0]/n_samples + cummulativeR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Neural Network Initialisation\n",
    "\n",
    "The input-data matrix X(5000, 400) is comprised of 5000 digit images 20 by 20 pixels (400 pixels).<br>\n",
    "The output-data vector Y(5000,1) is comprised of 5000 assigned digits (1 through 10; 10 represents figure '0').<br>\n",
    "The neural network in this work has 1 input layer (400 neurons), one hidden layer (25 neurons) and an output layer (10 neurons).\n",
    "To initialise a simple neural network, one has to do the following:\n",
    "1. set the number of neurons in every layer (including input and output layers)\n",
    "2. extract and flatten input matrix X\n",
    "3. transform output Y\n",
    "3. initialise Beat matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "# Set number of neurons in every layer (including input and output layers)\n",
    "layer = 400, 25, 10\n",
    "# Extract and flatten input matrix X\n",
    "X, y = get_data(file_path)\n",
    "n_samples, n_variables = X.shape\n",
    "X_flat = X.flatten()\n",
    "yUnique = np.unique(y)\n",
    "# Initialise Beat matrix\n",
    "β_test = flatten_β((weights['Theta1'], weights['Theta2']))\n",
    "β_initial = flatten_β(get_β(layer))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "for β in get_β(layer): print(β.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Forward-Propagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either transformed Y or y together with yUnique can be suplied to a function\n",
    "# Y = np.array([yUnique]* y.shape[0]) == y\n",
    "# print(Y[0:0+1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "print(forward_propagation(β_test, layer, X_flat, n_samples)[1].shape)\n",
    "print(forward_propagation(β_test, layer, X[0:0+1,:], 1)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(1, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2876291651613188"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0][None,:].shape)\n",
    "# cost_function(β_test, layer, X.flatten(), n_samples, y, yUnique, λ = 0.)\n",
    "cost_function(β_test, layer, X[0:5000][None,:].flatten(), 5000, y, yUnique, λ = 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Cost-Function Test\n",
    "The outputs of the cost_function should be as follows:<br\\>\n",
    "β_test, X, λ=0. — 0.287629 (Andrew Ng)<br\\>\n",
    "β_test, X, λ=1. — 0.383770 (Andrew Ng)<br\\>\n",
    "β_test, X, λ=0. — 0.0345203898838<br\\>\n",
    "β_initial, X, λ=1. — 65.5961451562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2876291651613188\n",
      "0.38448779624289386\n",
      "0.03452038988383936\n",
      "65.5961451562497\n"
     ]
    }
   ],
   "source": [
    "print(cost_function(β_test, layer, X_flat, n_samples, y, yUnique, λ = 0.))\n",
    "print(cost_function(β_test, layer, X_flat, n_samples, y, yUnique, λ = 1.))\n",
    "print(cost_function(β_test, layer, X[0][None,:].flatten(), 1, y, yUnique, λ = 0.))\n",
    "print(cost_function(β_initial, layer, X_flat, n_samples, y, yUnique, λ = 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Back Propagation\n",
    "$\\delta^l = H^l - Y$<br>\n",
    "$\\delta^{l-1} = (\\beta^{l-1})^T\\delta^l\\cdot g'(h^{l-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(β_flat, layer, X_flat, n_samples, y, yUnique):\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    β_set = reshape_β(β_flat, layer)\n",
    "\n",
    "    deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "    H = forward_propagation(β_flat, layer, X_flat, n_samples)\n",
    "#     error matrix E(5000, 10)\n",
    "    E = H[len(layer)-2] - Y\n",
    "    for l in reversed(range(len(layer)-1)):\n",
    "        E = np.dot(E*get_sigmoid_gradient(H[l]), β_set[l])[:,1:]\n",
    "        deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "    flatDelta = flatten_β(deltaSet)\n",
    "    return β_flat + flatDelta/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 25)\n",
      "(5000, 400)\n",
      "2\n",
      "(25, 401)\n",
      "(10, 26)\n",
      "(10285,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14300247142022265"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "# print(Y.shape)\n",
    "β_set = reshape_β(β_test, layer)\n",
    "# print(len(β_set))\n",
    "deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "H = forward_propagation(β_test, layer, X_flat, n_samples)\n",
    "# print (len(H))\n",
    "#     error matrix E(5000, 10)\n",
    "E = H[len(layer)-2] - Y\n",
    "# print(E.shape)\n",
    "for l in reversed(range(len(layer)-1)):\n",
    "    E = np.dot(E*get_sigmoid_gradient(H[l]), β_set[l])[:,1:]\n",
    "    print(E.shape)\n",
    "    deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "print(len(deltaSet))\n",
    "print(deltaSet[0].shape)\n",
    "print(deltaSet[1].shape)\n",
    "flatDelta = flatten_β(deltaSet)\n",
    "print(β_test.shape)\n",
    "f = β_test + flatDelta/n_samples\n",
    "f[3915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285,)\n",
      "0.14300247142022265\n",
      "-77.68979297139275\n",
      "0.32830159921277324\n"
     ]
    }
   ],
   "source": [
    "β_initial = flatten_β(get_β(layer))\n",
    "a = back_propagation(β_test, layer, X_flat, n_samples, y, yUnique)\n",
    "print(a.shape)\n",
    "print(a[3915])\n",
    "print(np.sum(a))\n",
    "print(cost_function(a,layer, X_flat, n_samples, y, yUnique, λ = 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7014 \t 3.416238375519853e-05 \t -0.0281727493987568\n",
      "3905 \t 1.523521031554509e-06 \t 0.016395658791554347\n",
      "1589 \t -6.722464251929239e-06 \t 0.022441850182514538\n",
      "7949 \t -0.0001419845119254859 \t 0.45346675743341425\n",
      "9639 \t 1.564271212561863e-06 \t -0.005225646712133616\n",
      "33 \t 4.110603524232204e-06 \t -0.01368051128447383\n",
      "6058 \t 2.2626955864524234e-07 \t -0.0007352434658607858\n",
      "1981 \t 3.0074051582396066e-06 \t -0.008847181911195554\n",
      "611 \t 5.569122940585203e-06 \t -0.027306578772908468\n",
      "3848 \t -5.17336948280267e-05 \t 0.17538764386857092\n"
     ]
    }
   ],
   "source": [
    "def check_gradient(β_flat, layer, X_flat, n_samples, y, yUnique, epsilon):\n",
    "    for i in np.random.randint(β_flat.size, size=10):\n",
    "        epsilonVector = np.zeros(β_flat.size)\n",
    "        epsilonVector[i] = epsilon\n",
    "        \n",
    "        gradient = back_propagation(β_flat, layer, X_flat, n_samples, y, yUnique)\n",
    "        \n",
    "        βPlus = βMinus = β_flat\n",
    "#         βPlus = β + epsilonVector\n",
    "        βPlus += epsilonVector\n",
    "        costPlus = cost_function(βPlus,layer, X, n_samples, y, yUnique, λ = 0.)\n",
    "#         βMinus = β - epsilonVector\n",
    "        βMinus -= epsilonVector\n",
    "        costMinus = cost_function(βMinus,layer, X, n_samples, y, yUnique, λ = 0.)\n",
    "        approximateGradient = (costPlus-costMinus)/(2*epsilon)\n",
    "        print (i, '\\t', approximateGradient, '\\t', gradient[i])\n",
    "\n",
    "epsilon = 0.0001\n",
    "check_gradient(β_test, layer, X_flat, n_samples, y, yUnique, epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_β_1(β_flat, X_flat, n_samples, y, yUnique, λ=0.):\n",
    "\n",
    "    β_optimised = optimize.minimize(cost_function, β_flat, args=(layer, X_flat, n_samples, y, yUnique),\n",
    "                                      method=None, jac=back_propagation, options={'maxiter':50})\n",
    "\n",
    "#     β_optimised = optimize.fmin_cg(cost_function, fprime=back_propagation, x0=β_flat,\n",
    "#                                      args=(layer, X_flat, n_samples, y, yUnique),\n",
    "#                                      maxiter=50,disp=True,full_output=True)\n",
    "    return(β_optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_β_2(β_flat, X_flat, n_samples, y, yUnique, λ=0.):\n",
    "\n",
    "#     β_optimised = optimize.minimize(cost_function, β_flat, args=(layer, X_flat, n_samples, y, yUnique),\n",
    "#                                       method=None, jac=back_propagation, options={'maxiter':50})\n",
    "\n",
    "    β_optimised = optimize.fmin_cg(cost_function, fprime=back_propagation, x0=β_flat,\n",
    "                                     args=(layer, X_flat, n_samples, y, yUnique),\n",
    "                                     maxiter=50,disp=True,full_output=True)\n",
    "    return(β_optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = optimise_β_1(β_initial, X_flat, n_samples, y, yUnique, λ=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.279260\n",
      "         Iterations: 2\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 78\n"
     ]
    }
   ],
   "source": [
    "b = optimise_β_2(β_initial, X_flat, n_samples, y, yUnique, λ=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_control(β_optimised, layer, X_flat, n_samples, y, yUnique, λ = 0.):\n",
    "    X = X_flat.reshape(n_samples,-1)\n",
    "    yAssignmentVector = []\n",
    "    misAssignedIndex = []\n",
    "    for n in range(n_samples):\n",
    "        x = X[n]\n",
    "        yAssignment =  np.argmax(forward_propagation(β_optimised, layer, X[n], 1)[1]) + 1\n",
    "        if yAssignment == y[n]:\n",
    "            yAssignmentVector += [True]\n",
    "        else:\n",
    "            yAssignmentVector += [False]\n",
    "            misAssignedIndex += [n]\n",
    "    return (sum(yAssignmentVector)/n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neuralNetworkClassifier(, X_flat, n_samples, y, yUnique, λ=0.)\n",
    "quality_control(a['x'], layer, X_flat, n_samples, y, yUnique, λ = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_control(b[0], layer, X_flat, n_samples, y, yUnique, λ = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_andrew_ng_2_python",
   "language": "python",
   "name": "venv_andrew_ng_2_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
