{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.image import NonUniformImage\n",
    "from matplotlib import cm\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n",
      "(10, 26)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "pathToDataFile = 'ex4data1.mat'\n",
    "data = loadmat(pathToDataFile)\n",
    "pathToWeightsFile = 'ex4weights.mat'\n",
    "weights = loadmat(pathToWeightsFile)\n",
    "print(weights['Theta1'].shape)\n",
    "print(weights['Theta2'].shape)\n",
    "print(type(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Nural Network\n",
    "## 1.1 Forward Porpagation\n",
    "<img src=\"forwardPropagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(pathToDataFile):\n",
    "    data = loadmat(pathToDataFile)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "def generateBeta(layer):\n",
    "    '''Generate beta-matrix for every layer in Neural Network'''\n",
    "    betaSet = ()\n",
    "    for i in range(len(layer)-1):\n",
    "#         recommendation from Andrew Ng window is ±(6/(inLayer + outLayer))**0.5\n",
    "        low, high = -(6/(layer[i]+layer[i+1]))**0.5, (6/(layer[i]+layer[i+1]))**0.\n",
    "        betaSet += (np.random.uniform(low,high,(layer[i+1], layer[i]+1)),)\n",
    "#         betaSet += (np.zeros((outLayer, inLayer+1)),)\n",
    "    return betaSet\n",
    "\n",
    "def flattenBeta(betaSet):\n",
    "    flatBeta = betaSet[0].flatten()\n",
    "    for beta in betaSet[1:]:\n",
    "        flatBeta = np.concatenate((flatBeta, beta.flatten()), axis=-1)\n",
    "    return flatBeta\n",
    "\n",
    "def reshapeBeta(Beta, layer):\n",
    "    splitIndex = 0\n",
    "    splitIndices = []\n",
    "    for i in range(len(layer)-2):\n",
    "        splitIndex += (layer[i]+1)*layer[i+1]\n",
    "        splitIndices += [splitIndex]\n",
    "    splitBeta = np.split(Beta, splitIndices)\n",
    "    reshapedBeta = ()\n",
    "    for i in range(len(splitBeta)):\n",
    "        reshapedBeta += (splitBeta[i].reshape(layer[i+1],layer[i]+1),)\n",
    "    return reshapedBeta\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forwardPropagation(flatBeta, layer, flatX, sampleSize):\n",
    "    '''Forward Propagation is the hypothesis function for Neural Networks'''\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "#     H_0 (5000, 400)\n",
    "    H = flatX.reshape(sampleSize, -1)\n",
    "#     Z_H = ()\n",
    "    H_byLayer = ()\n",
    "    for beta in betaSet:\n",
    "#         print(H.shape)\n",
    "#         Z_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        Z = np.dot(np.insert(H, 0, 1, axis=1), beta.T)\n",
    "#         H_l (5000, k_l); l is the number of layers [0, ...,l]; k is the number of neurons in a layer l [1,...,k]\n",
    "        H = sigmoid(Z)\n",
    "#         Z_H += ((Z, H),)\n",
    "        H_byLayer += (H,)\n",
    "#     H_2 (5000, 10)\n",
    "    return H_byLayer\n",
    "\n",
    "def sigmoidGradient(Z):\n",
    "    return sigmoid(Z)*(1-sigmoid(Z))\n",
    "\n",
    "def costFunction(flatBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    X = flatX.reshape(sampleSize, -1)\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "    J = 0\n",
    "    for n in range(sampleSize):\n",
    "        x_n = X[n:n+1,:]\n",
    "        y_n = Y[n:n+1,:]\n",
    "#         hypothesis vector h_n(1, 10)\n",
    "        h_n = forwardPropagation(flatBeta, layer, x_n, 1)[len(betaSet)-1]\n",
    "#         cost function scalar j_n(1, 1) = y_n(1, 10)*h_n.T(10, 1)\n",
    "        j_n = (- np.dot(y_n, np.log(h_n).T) - np.dot((1-y_n), np.log(1-h_n).T))\n",
    "        J += j_n\n",
    "#     regularisation term (R)\n",
    "    cummulativeR = 0\n",
    "    for beta in betaSet:\n",
    "        cummulativeR += np.sum(beta*beta) #element-wise multiplication\n",
    "    cummulativeR *= iLambda/(2*sampleSize)\n",
    "    return J[0][0]/sampleSize + cummulativeR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Neural Network Initialisation\n",
    "\n",
    "The input-data matrix X(5000, 400) is comprised of 5000 digit images 20 by 20 pixels (400 pixels).<br>\n",
    "The output-data vector Y(5000,1) is comprised of 5000 assigned digits (1 through 10; 10 represents figure '0').<br>\n",
    "The neural network in this work has 1 input layer (400 neurons), one hidden layer (25 neurons) and an output layer (10 neurons).\n",
    "To initialise a simple neural network, one has to do the following:\n",
    "1. set the number of neurons in every layer (including input and output layers)\n",
    "2. extract and flatten input matrix X\n",
    "3. transform output Y\n",
    "3. initialise Beat matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "# Set number of neurons in every layer (including input and output layers)\n",
    "layer = 400, 25, 10\n",
    "# Extract and flatten input matrix X\n",
    "X, y = getData(pathToDataFile)\n",
    "sampleSize, numVariables = X.shape\n",
    "flatX = X.flatten()\n",
    "yUnique = np.unique(y)\n",
    "# Initialise Beat matrix\n",
    "betaTest = flattenBeta((weights['Theta1'], weights['Theta2']))\n",
    "betaInitial = flattenBeta(generateBeta(layer))\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "for beta in generateBeta(layer): print(beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Forward-Propagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either transformed Y or y together with yUnique can be suplied to a function\n",
    "# Y = np.array([yUnique]* y.shape[0]) == y\n",
    "# print(Y[0:0+1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "print(forwardPropagation(betaTest, layer, flatX, sampleSize)[1].shape)\n",
    "print(forwardPropagation(betaTest, layer, X[0:0+1,:], 1)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(1, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28762916516131876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0][None,:].shape)\n",
    "# costFunction(betaTest, layer, X.flatten(), sampleSize, y, yUnique, iLambda = 0.)\n",
    "costFunction(betaTest, layer, X[0:5000][None,:].flatten(), 5000, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Cost-Function Test\n",
    "The outputs of the costFunction should be as follows:<br\\>\n",
    "betaTest, X, iLambda=0. — 0.287629 (Andrew Ng)<br\\>\n",
    "betaTest, X, iLambda=1. — 0.383770 (Andrew Ng)<br\\>\n",
    "betaTest, X, iLambda=0. — 0.0345203898838<br\\>\n",
    "betaInitial, X, iLambda=1. — 65.5961451562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n",
      "0.384487796243\n",
      "0.0345203898838\n",
      "65.5961451562\n"
     ]
    }
   ],
   "source": [
    "print(costFunction(betaTest, layer, flatX, sampleSize, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(betaTest, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))\n",
    "print(costFunction(betaTest, layer, X[0][None,:].flatten(), 1, y, yUnique, iLambda = 0.))\n",
    "print(costFunction(betaInitial, layer, flatX, sampleSize, y, yUnique, iLambda = 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Back Propagation\n",
    "$\\delta^l = H^l - Y$<br>\n",
    "$\\delta^{l-1} = (\\beta^{l-1})^T\\delta^l\\cdot g'(h^{l-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation(flatBeta, layer, flatX, sampleSize, y, yUnique):\n",
    "    Y = np.array([yUnique]* y.shape[0]) == y\n",
    "    betaSet = reshapeBeta(flatBeta, layer)\n",
    "\n",
    "    deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "    H = forwardPropagation(flatBeta, layer, flatX, sampleSize)\n",
    "#     error matrix E(5000, 10)\n",
    "    E = H[len(layer)-2] - Y\n",
    "    for l in reversed(range(len(layer)-1)):\n",
    "        E = np.dot(E*sigmoidGradient(H[l]), betaSet[l])[:,1:]\n",
    "        deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "    flatDelta = flattenBeta(deltaSet)\n",
    "    return flatBeta + flatDelta/sampleSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 25)\n",
      "(5000, 400)\n",
      "2\n",
      "(25, 401)\n",
      "(10, 26)\n",
      "(10285,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14300247142022265"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([yUnique]* y.shape[0]) == y\n",
    "# print(Y.shape)\n",
    "betaSet = reshapeBeta(betaTest, layer)\n",
    "# print(len(betaSet))\n",
    "deltaSet = ()\n",
    "#     hypothesis matrix E(5000, 10)\n",
    "H = forwardPropagation(betaTest, layer, flatX, sampleSize)\n",
    "# print (len(H))\n",
    "#     error matrix E(5000, 10)\n",
    "E = H[len(layer)-2] - Y\n",
    "# print(E.shape)\n",
    "for l in reversed(range(len(layer)-1)):\n",
    "    E = np.dot(E*sigmoidGradient(H[l]), betaSet[l])[:,1:]\n",
    "    print(E.shape)\n",
    "    deltaSet = (np.dot(H[l].T, np.insert(E, 0, 1, axis=1)),) + deltaSet\n",
    "print(len(deltaSet))\n",
    "print(deltaSet[0].shape)\n",
    "print(deltaSet[1].shape)\n",
    "flatDelta = flattenBeta(deltaSet)\n",
    "print(betaTest.shape)\n",
    "f = betaTest + flatDelta/sampleSize\n",
    "f[3915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285,)\n",
      "0.14300247142\n",
      "-77.6897929714\n",
      "0.328301599213\n"
     ]
    }
   ],
   "source": [
    "betaInitial = flattenBeta(generateBeta(layer))\n",
    "a = backPropagation(betaTest, layer, flatX, sampleSize, y, yUnique)\n",
    "print(a.shape)\n",
    "print(a[3915])\n",
    "print(np.sum(a))\n",
    "print(costFunction(a,layer, flatX, sampleSize, y, yUnique, iLambda = 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3915 \t -5.16694076413e-05 \t 0.14300247142\n",
      "2553 \t 0.000130317904801 \t -0.393868746806\n",
      "7889 \t -8.70829020005e-06 \t -0.00242495412\n",
      "2259 \t -6.83055711992e-05 \t 0.144925981932\n",
      "3342 \t 5.38021305285e-05 \t -0.178667553118\n",
      "5405 \t -9.52503684259e-05 \t 0.206407475658\n",
      "5084 \t -4.85511891801e-05 \t 0.172304406568\n",
      "859 \t 1.37250885657e-05 \t -0.0466439352383\n",
      "3619 \t 8.66365312824e-08 \t -0.000303325624571\n",
      "5647 \t 1.31436750372e-06 \t -0.0043613719981\n"
     ]
    }
   ],
   "source": [
    "def gradientCheck(flatBeta, layer, flatX, sampleSize, y, yUnique, epsilon):\n",
    "    for i in np.random.randint(flatBeta.size, size=10):\n",
    "        epsilonVector = np.zeros(flatBeta.size)\n",
    "        epsilonVector[i] = epsilon\n",
    "        \n",
    "        gradient = backPropagation(flatBeta, layer, flatX, sampleSize, y, yUnique)\n",
    "        \n",
    "        betaPlus = betaMinus = flatBeta\n",
    "#         betaPlus = beta + epsilonVector\n",
    "        betaPlus += epsilonVector\n",
    "        costPlus = costFunction(betaPlus,layer, X, sampleSize, y, yUnique, iLambda = 0.)\n",
    "#         betaMinus = beta - epsilonVector\n",
    "        betaMinus -= epsilonVector\n",
    "        costMinus = costFunction(betaMinus,layer, X, sampleSize, y, yUnique, iLambda = 0.)\n",
    "        approximateGradient = (costPlus-costMinus)/(2*epsilon)\n",
    "        print (i, '\\t', approximateGradient, '\\t', gradient[i])\n",
    "\n",
    "epsilon = 0.0001\n",
    "gradientCheck(betaTest, layer, flatX, sampleSize, y, yUnique, epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def betaOptimisation_1(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "    optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                      method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "#     optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "#                                      args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                      maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaOptimisation_2(flatBeta, flatX, sampleSize, y, yUnique, iLambda=0.):\n",
    "\n",
    "#     optimisedBeta = optimize.minimize(costFunction, flatBeta, args=(layer, flatX, sampleSize, y, yUnique),\n",
    "#                                       method=None, jac=backPropagation, options={'maxiter':50})\n",
    "\n",
    "    optimisedBeta = optimize.fmin_cg(costFunction, fprime=backPropagation, x0=flatBeta,\n",
    "                                     args=(layer, flatX, sampleSize, y, yUnique),\n",
    "                                     maxiter=50,disp=True,full_output=True)\n",
    "    return(optimisedBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = betaOptimisation_1(betaInitial, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = betaOptimisation_2(betaInitial, flatX, sampleSize, y, yUnique, iLambda=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualityControl(optimisedBeta, layer, flatX, sampleSize, y, yUnique, iLambda = 0.):\n",
    "    X = flatX.reshape(sampleSize,-1)\n",
    "    yAssignmentVector = []\n",
    "    misAssignedIndex = []\n",
    "    for n in range(sampleSize):\n",
    "        x = X[n]\n",
    "        yAssignment =  np.argmax(forwardPropagation(optimisedBeta, layer, X[n], 1)[1]) + 1\n",
    "        if yAssignment == y[n]:\n",
    "            yAssignmentVector += [True]\n",
    "        else:\n",
    "            yAssignmentVector += [False]\n",
    "            misAssignedIndex += [n]\n",
    "    return (sum(yAssignmentVector)/sampleSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuralNetworkClassifier(, flatX, sampleSize, y, yUnique, iLambda=0.)\n",
    "qualityControl(a['x'], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualityControl(b[0], layer, flatX, sampleSize, y, yUnique, iLambda = 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_andrew_ng_2_python",
   "language": "python",
   "name": "venv_andrew_ng_2_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
