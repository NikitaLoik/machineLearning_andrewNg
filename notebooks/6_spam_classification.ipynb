{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spamTestFile = 'spamTest.mat'\n",
    "spamTrainFile = 'spamTrain.mat'\n",
    "\n",
    "eMail_1 = 'emailSample1.txt'\n",
    "eMail_2 = 'emailSample2.txt'\n",
    "spam_1 = 'spamSample1.txt'\n",
    "spam_2 = 'spamSample2.txt'\n",
    "\n",
    "vocabFile = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamTestData = loadmat(spamTestFile)\n",
    "spamTrainData = loadmat(spamTrainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])\n",
      "(4000, 1899)\n",
      "(4000, 1)\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'Xtest', 'ytest'])\n",
      "(1000, 1899)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print (spamTrainData.keys())\n",
    "print (spamTrainData['X'].shape)\n",
    "print (spamTrainData['y'].shape)\n",
    "print (spamTestData.keys())\n",
    "print (spamTestData['Xtest'].shape)\n",
    "print (spamTestData['ytest'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Extraction and Transformation\n",
    "## 1.1 Import dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importVocab(vocabFile):\n",
    "    wordList = csv.reader(open(vocabFile), delimiter=\"\\t\")\n",
    "    vocab ={}\n",
    "    for row in wordList:\n",
    "        vocab[row[0]] = row[1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Map Indeces onto e-Mails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordStemming(word):\n",
    "    '''Remove suffixes -e, -s, -es, -ed, -ing from the words in e-mail'''\n",
    "    wordSuffixes = ('s', 'es', 'ed', 'ing')\n",
    "    if len(word)>2:\n",
    "        for suffix in wordSuffixes:\n",
    "            if word.endswith(suffix):\n",
    "                return wordStemming(word[:-len(suffix)])\n",
    "    return word\n",
    "\n",
    "def preprocess_eMail(eMail_txt):\n",
    "    '''preprocess the e-mail, so that it matches the vocabulary'''\n",
    "    eMail = open(eMail_txt, 'r')\n",
    "#     converting to lower case and stripping the punctuation\n",
    "    wordList = [word.strip(string.punctuation).lower() for word in eMail.read().split()]\n",
    "\n",
    "    for i, word in enumerate(wordList):\n",
    "#         word stemming\n",
    "        wordList[i] = wordStemming(word)\n",
    "#         normalise URLs\n",
    "        if 'http' in word or 'www' in word:\n",
    "            wordList[i] = 'httpaddr'\n",
    "#         normalise eMail addresses\n",
    "        elif '@' in word:\n",
    "            wordList[i] = 'emailaddr'\n",
    "#         normalise numbers\n",
    "        elif any(char.isdigit() for char in word):\n",
    "            wordList[i] = 'number'\n",
    "#         normalise dollar sign\n",
    "        elif '$' in word:\n",
    "            wordList[i] = 'dollar'\n",
    "#     remove empty strings\n",
    "    wordList = [word for word in wordList if word != '']\n",
    "    return wordList\n",
    "\n",
    "def extractFeatures(vocabFile, eMail_txt):\n",
    "    vocab = importVocab(vocabFile)\n",
    "    invVocab = {v: int(k) for k, v in vocab.items()}\n",
    "    wordList = preprocess_eMail(eMail_txt)\n",
    "    wordIndexList = [invVocab[word] for word in wordList if word in invVocab]\n",
    "    uniqueIndex = list(set(wordIndexList))\n",
    "    eMailVector = np.zeros(len(vocab))\n",
    "    eMailVector[[uniqueIndex]] = 1\n",
    "    return eMailVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(extractFeatures(vocabFile, spam_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = spamTrainData['X']\n",
    "y_train = spamTrainData['y']\n",
    "X_test = spamTestData['Xtest']\n",
    "y_test = spamTestData['ytest']\n",
    "\n",
    "sigma = 1\n",
    "gaussianSVM = svm.SVC(C=1, kernel='rbf', gamma=sigma**(-2))\n",
    "gaussianSVM.fit(X_train, y_train.flatten())\n",
    "\n",
    "a = gaussianSVM.decision_function(X_train).reshape(y_train.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0 \n",
      "Precision = 1.0 \n",
      "Recall = 1.0\n"
     ]
    }
   ],
   "source": [
    "tP = np.sum(y_train*b)\n",
    "fP = np.sum(y_train-b==-1)\n",
    "fN = np.sum(y_train-b==1)\n",
    "accuracy = (y_train.shape[0] - fP - fN)/y_train.shape[0]\n",
    "precision  = tP/(tP+fP)\n",
    "recall  = tP/(tP+fN)\n",
    "print(\"Accuracy\", accuracy, \"\\nPrecision =\", precision, \"\\nRecall =\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = gaussianSVM.decision_function(X_test).reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape\n",
    "d = c>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.798 \n",
      "Precision = 1.0 \n",
      "Recall = 0.344155844156\n"
     ]
    }
   ],
   "source": [
    "tP = np.sum(y_test*d)\n",
    "fP = np.sum(y_test-d==-1)\n",
    "fN = np.sum(y_test-d==1)\n",
    "accuracy = (y_test.shape[0] - fP - fN)/y_test.shape[0]\n",
    "precision  = tP/(tP+fP)\n",
    "recall  = tP/(tP+fN)\n",
    "print(\"Accuracy\", accuracy, \"\\nPrecision =\", precision, \"\\nRecall =\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
